# MetalMind: A Knowledge Graph + RAG System for Metal AM

MetalMind is a human-centric, multimodal knowledge system that integrates a large-scale knowledge graph (KG), retrieval-augmented generation (RAG) to support metal additive manufacturing (AM) training and decision-making. 

---

## ðŸ”§ Features

- ðŸ“„ **PDF-to-Knowledge Graph**: Converts metal AM manuals into structured knowledge using GLM-4V-9B.
- ðŸ§  **Entity & Relation Extraction**: GLM-4V-9B-based pipeline for schema-free âž schema-based KG creation.
- ðŸ”— **Neo4j Graph Storage**: All nodes and relationships stored in a traversable graph database.
- ðŸ” **Three-Mode Retrieval System**:
  - **Vector Retrieval** using FAISS and entity embeddings
  - **Graph Traversal Retrieval** via Neo4j
  - **Hybrid Retrieval** combining both
- ðŸ–¼ï¸ **Image Retrieval via CLIP + LoRA**: Fine-tuned CLIP model with figure captions, enhancing diagram-based explanation and multimodal reasoning.
- ðŸ“Š **Evaluation Metrics**: Support for faithfulness, answer precision, rubric scores, and token-efficiency tracking.
- ðŸŒ **Streamlit Web App**: Visual interactive front-end for testing text/image queries.

---

## ðŸ“ Project Structure

```bash
.
â”œâ”€â”€ preprocess.py              # PDF to markdown + chunking
â”œâ”€â”€ build_kg_neo4j.py          # GLM-4V-9B-based KG construction, writes to Neo4j
â”œâ”€â”€ postprocess.py             # Entity cleanup, deduplication
â”œâ”€â”€ neo4j_handler.py           # Neo4j API abstraction
â”œâ”€â”€ rag_vector.py              # Vector-based retrieval
â”œâ”€â”€ rag_graph.py               # Graph-based retrieval
â”œâ”€â”€ rag_hybrid.py              # Combined hybrid mode
â”œâ”€â”€ clip_lora_train.py         # Train LoRA on CLIP with figure captions
â”œâ”€â”€ clip_lora_infer.py         # Use LoRA-CLIP to retrieve images
â”œâ”€â”€ evaluate.py                # Retrieval evaluation script
â”œâ”€â”€ app.py                     # Streamlit app
â”œâ”€â”€ full_pipeline.py           # Optional: run all modules in sequence
â””â”€â”€ README.md
```

---

## ðŸš€ Getting Started

### 1. Setup Environment
```bash
conda create -n metalmind python=3.10
conda activate metalmind
pip install -r requirements.txt
```

### 2. Prepare Data
- Place your machine manual PDF in the root directory (e.g., `AM400.pdf`)
- Prepare a CSV file `extracted_figures_with_paths.csv` with columns:
  - `Local Image Path`
  - `Caption`

### 3. Run Pipeline
```bash
python preprocess.py
python build_kg_neo4j.py
python postprocess.py
```

### 4. Train LoRA-enhanced CLIP
```bash
python clip_lora_train.py
```

### 5. Launch Web App
```bash
streamlit run app.py
```

---

## ðŸ¤– Using GLM-4V-9B
This project relies on [`THUDM/glm-4v-9b`](https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7) for all natural language and multimodal tasks. You can:

- Use it via HuggingFace Transformers (`transformers>=4.44`)
- Deploy locally or call through OpenBMB, InternLM or ChatGLM API-compatible services
- Supports both caption generation and instruction-style prompting for entity-relation extraction

---

## ðŸ§ª Evaluation
```bash
python evaluate.py
```
Customize the metrics and query list in the script.

---

## ðŸ“š Requirements
- Python 3.10+
- Neo4j
- FAISS
- HuggingFace Transformers (GLM-4V-9B)
- PyMuPDF
- sentence-transformers
- peft, bitsandbytes
- Streamlit, scikit-learn, pandas, tqdm

---

## ðŸ“Œ Citation & Acknowledgement
- GLM-4V-9B: [https://huggingface.co/collections/THUDM/glm-4](https://huggingface.co/collections/THUDM/glm-4)
- Inspired by Fan et al. (2025): "MetalMind: A Knowledge Graph-Driven Human-Centric Knowledge System for Metal Additive Manufacturing"

---
